---
title: "Practical Machine Learning Project"
author: "Karuna Raghuwanshi"
date: "04/05/2020"
output:
  html_document: default
  word_document: default
---


## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The main goal of the project is to predict the manner in which 6 participants performed some exercise as described below. This is the “classe” variable in the training set.Three machine learning models are applied to the 20 test cases available in the test data and the predictions are submitted in appropriate format to the Course Project Prediction Quiz for automated grading.


## Data Loading and Cleaning

The next step is loading the dataset from the URL provided above. The training dataset is then partinioned in 2 to create a Training set (70% of the data) for the modeling process and a Test set (with the remaining 30%) for the validations. The testing dataset will be used for the quiz results generation.


```{r}
# Loading the Libraries
rm(list=ls())                
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(corrplot)
library(gbm)

set.seed(12345)

# downloading the data
testing  <- read.csv("pml-testing.csv", head=TRUE)
training <- read.csv("pml-training.csv", head=TRUE)

# To create a partition with the training dataset 

inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
TrainSet <- training[inTrain, ]
TestSet  <- training[-inTrain, ]
dim(TrainSet)
dim(TestSet)

# To remove variables with Nearly Zero Variance from the dataset

NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV]
TestSet  <- TestSet[, -NZV]
dim(TrainSet)
dim(TestSet)
# To remove variables that are mostly NA

NA_Values    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, NA_Values==FALSE]
TestSet  <- TestSet[, NA_Values==FALSE]
dim(TrainSet)
dim(TestSet)

# To remove identification only variables (columns 1 to 5)

TrainSet <- TrainSet[, -(1:5)]
TestSet  <- TestSet[, -(1:5)]
dim(TrainSet)
dim(TestSet)
```
After cleaning , the number of variables for the analysis has been reduced to 54 only.

## Correlation Analysis

Before proceeding to the modeling procedures, a correlation among variables is analysed

```{r}
corMatrix <- cor(TrainSet[, -54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))

```

This plot shows highly correlated variables with dark colors. 

## Prediction Model Building

Below three models will be applied to model the regressions (in the Train dataset)
1. Random Forests
2. Decision Tree 
3. Generalized Boosted Model

A Confusion Matrix is plotted at the end of each analysis to better visualize the accuracy of the models.
The model with higher accuracy(Best Fit) will be  applied to the Test dataset

### 1. Random Forest Model

```{r}
set.seed(12345)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
Model_RF <- train(classe ~ ., data=TrainSet, method="rf",
                          trControl=controlRF)
Model_RF$finalModel
```

### prediction on Test dataset

```{r}
predictRandForest <- predict(Model_RF, newdata=TestSet)
Confusion_Mat_RF <- confusionMatrix(predictRandForest, TestSet$classe)
Confusion_Mat_RF

plot(Confusion_Mat_RF$table, col = Confusion_Mat_RF$byClass, 
     main = paste("Random Forest - Accuracy =",
                  round(Confusion_Mat_RF$overall['Accuracy'], 4)))

```

### 2. Decision Trees Model

```{r}
set.seed(12345)
Model_DecisionTree <- rpart(classe ~ ., data=TrainSet, method="class")
fancyRpartPlot(Model_DecisionTree)
```


### prediction on Test dataset
```{r}
predictDecTree <- predict(Model_DecisionTree, newdata=TestSet, type="class")
Confusion_Mat_DT <- confusionMatrix(predictDecTree, TestSet$classe)
Confusion_Mat_DT

plot(Confusion_Mat_DT$table, col = Confusion_Mat_DT$byClass, 
     main = paste("Decision Tree - Accuracy =",
                  round(Confusion_Mat_DT$overall['Accuracy'], 4)))

```

### 3. Generalized Boosted Model

```{r}
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
Model_GB  <- train(classe ~ ., data=TrainSet, method = "gbm",
                    trControl = controlGBM, verbose = FALSE)
Model_GB$finalModel
```
### prediction on Test dataset

```{r}
predictGBM <- predict(Model_GB, newdata=TestSet)
confMatGBM <- confusionMatrix(predictGBM, TestSet$classe)
confMatGBM

plot(confMatGBM$table, col = confMatGBM$byClass, 
     main = paste("GBM - Accuracy =", round(confMatGBM$overall['Accuracy'], 4)))

```

## Best Model to test data
The accuracy of the 3 regression modeling methods above are:

Random Forest : 0.9963
Decision Tree : 0.7368
GBM : 0.9839
Hence, the Random Forest model will be applied to predict testing dataset:
```{r}
predictTEST <- predict(Model_RF, newdata=testing)
predictTEST
```

